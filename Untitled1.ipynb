{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hardiksingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/hardiksingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/hardiksingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/hardiksingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/hardiksingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/hardiksingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hardiksingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatize=WordNetLemmatizer()\n",
    "\n",
    "'''\n",
    "importing porter stemmer for stemming to  the root word\n",
    "most generally used stemmers are:\n",
    "1. Porter Stemmer\n",
    "2. Snowball Stemmer\n",
    "3. Lancaster Stemmer\n",
    "\n",
    "'''\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#importing libraries\n",
    "import numpy\n",
    "import tflearn as tfl\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day', 'Whats up'], 'responses': ['Hello!', 'Welcome to Sabertooth technologies,Good to see you again!', 'Hi there, how can I help?'], 'context_set': ''}, {'tag': 'goodbye', 'patterns': ['cya', 'See you later', 'Goodbye', 'I am Leaving', 'Have a Good day'], 'responses': ['Sad to see you go :(', 'Talk to you later', 'Goodbye!'], 'context_set': ''}, {'tag': 'age', 'patterns': ['how old', 'how old is tim', 'what is your age', 'how old are you', 'age?'], 'responses': ['I am 18 years old!', '18 years young!'], 'context_set': ''}, {'tag': 'name', 'patterns': ['what is your name', 'what should I call you', 'whats your name?'], 'responses': ['You can call me Tim.', \"I'm Tim!\", \"I'm Tim aka Tech With Tim.\"], 'context_set': ''}, {'tag': 'shop', 'patterns': ['Id like to buy something', 'whats on the menu', 'what do you reccommend?', 'could i get something to eat'], 'responses': ['We sell chocolate chip cookies for $2!', 'Cookies are on the menu!'], 'context_set': ''}, {'tag': 'hours', 'patterns': ['when are you guys open', 'what are your hours', 'hours of operation'], 'responses': ['We are open 7am-4pm Monday-Friday!'], 'context_set': ''}, {'tag': 'services', 'patterns': ['what do you specialize in', 'which language do you use'], 'responses': ['We specialize in all kinds of web development and machine learning services', 'we use django framework with python'], 'context_set': ''}, {'tag': 'uniqueness', 'patterns': ['what makes your product or service different', 'why should we invest in you'], 'responses': ['We serve the most unique products to our clients in a reasonable price like website development, app development', 'Please have a look at our website at https://www.sabertoothtech.in/'], 'context_set': ''}, {'tag': 'Information', 'patterns': ['were you able to find information you were looking for, on our website', 'did you find the information valuable'], 'responses': ['Yes, I was looking for someone that works in web services and Machine learning, I will contact you within sometime'], 'context_set': ''}, {'tag': 'Working hours', 'patterns': ['What are your working hours'], 'responses': ['We are available from 10am to 7pm'], 'context_set': ''}, {'tag': 'charge', 'patterns': ['How much do you charge for designing a simple website'], 'responses': ['We charge 1000 INR for a simple HTML website'], 'context_set': ''}]}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1.Stemming the words in the questions and answers file\n",
    "2.creating a one hot encoded matrix for words which exists in the matrix(because as we know,\n",
    "neural network is good with numbers rather than text) \n",
    "3.Finally converting the tensors into numpy array so that it could be trainedusing neural network\n",
    "        \n",
    "'''\n",
    "\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "    print(data)\n",
    "\n",
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, testing = pickle.load(f)\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    #words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    #words = sorted(list(set(words)))\n",
    "    \n",
    "    #We can use either of the lemmetaization or Stemming but generally lemmatization produces better root words so I am using it.\n",
    "    words = [lemmatizer.lemmatize(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    testing = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        testing.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    testing = numpy.array(testing)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hardiksingh/anaconda3/lib/python3.7/site-packages/tflearn/objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /Users/hardiksingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/hardiksingh/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /Users/hardiksingh/Desktop/chatbot/model.tflearn\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Designing a neural network model with tflearn library having:\n",
    "1. input_layer\n",
    "2. fully_connected_layers\n",
    "3. output_layer\n",
    "'''\n",
    "net = tfl.input_data(shape=[None, len(training[0])])\n",
    "net = tfl.fully_connected(net, 16)\n",
    "net = tfl.fully_connected(net, 16)\n",
    "net = tfl.fully_connected(net, 16)\n",
    "net = tfl.fully_connected(net, len(testing[0]), activation=\"softmax\")\n",
    "net = tfl.regression(net)\n",
    "\n",
    "#created model object to compile and fit the neural network model\n",
    "model = tfl.DNN(net)\n",
    "\n",
    "\n",
    "try:\n",
    "    #load saved model\n",
    "    model.load(\"model.tflearn\")\n",
    "\n",
    "\n",
    "except:\n",
    "    model.fit(training, testing, n_epoch=2000, batch_size=16, show_metric=True)\n",
    "    model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function generates a function which makes a one hot encoded matrix of 0's and 1's    \n",
    "def bag_of_words(a,words):\n",
    "    bag=[0 for _ in range(len(words))]\n",
    "    \n",
    "    a_words=nltk.word_tokenize(a)\n",
    "    a_words=[stemmer.stem(word.lower()) for word in a_words]\n",
    "    \n",
    "    \n",
    "    #This for loop creates 1 in the matrix where the words are found in english words set\n",
    "    for s in a_words:\n",
    "        for i,value in enumerate(words):\n",
    "            if value==s:\n",
    "                bag[i]=1\n",
    "                \n",
    "    return numpy.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's talk to the chatbot\n",
      "You:Hi\n",
      "Sorry, I didn't get that\n",
      "You:Hi, How are you\n",
      "Welcome to Sabertooth technologies,Good to see you again!\n",
      "You:what services you offer\n",
      "Sorry, I didn't get that\n",
      "You:What do you specialize in\n",
      "Sorry, I didn't get that\n",
      "You:quit\n"
     ]
    }
   ],
   "source": [
    "def talk():\n",
    "    print(\"Let's talk to the chatbot\")\n",
    "    while True:\n",
    "        inp=input(\"You:\")\n",
    "        if inp.lower()==\"quit\":\n",
    "            break\n",
    "    \n",
    "        predictions=model.predict([bag_of_words(inp,words)])[0]\n",
    "        predictions_index=numpy.argmax(predictions)\n",
    "        tag=labels[predictions_index]\n",
    "        \n",
    "        if predictions[predictions_index]>0.8:\n",
    "            for t in data[\"intents\"]:\n",
    "                if t['tag']==tag:\n",
    "                    responses=t['responses']\n",
    "                \n",
    "            print(random.choice(responses))\n",
    "        else:\n",
    "            print(\"Sorry, I didn't get that\")\n",
    "\n",
    "talk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
